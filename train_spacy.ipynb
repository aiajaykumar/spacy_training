{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import logging\n",
    "import os\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.scorer import Scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main_functions for model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n",
    "    try:\n",
    "        training_data = []\n",
    "        lines=[]\n",
    "        with open(dataturks_JSON_FilePath, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            data = json.loads(line)\n",
    "            text = data['content']\n",
    "            entities = []\n",
    "            for annotation in data['annotation']:\n",
    "                point = annotation['points'][0]\n",
    "                labels = annotation['label']\n",
    "                if not isinstance(labels, list):\n",
    "                    labels = [labels]\n",
    "                for label in labels:\n",
    "                    entities.append((point['start'], point['end'] + 1 ,label))\n",
    "            training_data.append((text, {\"entities\" : entities}))\n",
    "        return training_data\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process \" + dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_spacy():\n",
    "    TRAIN_DATA = convert_dataturks_to_spacy(\"/home/ajay/Downloads/new _resumes.json\")\n",
    "    nlp = spacy.blank('en')  # create blank Language class\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "         for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(100):\n",
    "            print(\"Statring iteration \" + str(itn))\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in TRAIN_DATA:\n",
    "                nlp.update([text],[annotations],drop=0.2,sgd=optimizer,losses=losses)\n",
    "        current_directory = \"/home/ajay/spacy_models/\"\n",
    "        final_directory = os.path.join(current_directory,'spacy_updated_model'+ \" \" +str(losses))\n",
    "        if not os.path.exists(final_directory):\n",
    "            os.makedirs(final_directory)\n",
    "            nlp.to_disk(final_directory)\n",
    "            print(\"Saved model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))\n",
      "Statring iteration 0\n",
      "Statring iteration 1\n",
      "Statring iteration 2\n",
      "Statring iteration 3\n",
      "Statring iteration 4\n",
      "Statring iteration 5\n",
      "Statring iteration 6\n",
      "Statring iteration 7\n",
      "Statring iteration 8\n",
      "Statring iteration 9\n",
      "Statring iteration 10\n",
      "Statring iteration 11\n",
      "Statring iteration 12\n",
      "Statring iteration 13\n",
      "Statring iteration 14\n",
      "Statring iteration 15\n",
      "Statring iteration 16\n",
      "Statring iteration 17\n",
      "Statring iteration 18\n",
      "Statring iteration 19\n",
      "Statring iteration 20\n",
      "Statring iteration 21\n",
      "Statring iteration 22\n",
      "Statring iteration 23\n",
      "Statring iteration 24\n",
      "Statring iteration 25\n",
      "Statring iteration 26\n",
      "Statring iteration 27\n",
      "Statring iteration 28\n",
      "Statring iteration 29\n"
     ]
    }
   ],
   "source": [
    "train_spacy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading model and testing with resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2 = spacy.load(\"your_model_file\")\n",
    "examples = textract.process(\"resume path\")\n",
    "examples = examples.decode()\n",
    "doc = nlp2(examples)\n",
    "for ent in doc.ents:\n",
    "    a = {ent.label_:ent.text}\n",
    "    print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
